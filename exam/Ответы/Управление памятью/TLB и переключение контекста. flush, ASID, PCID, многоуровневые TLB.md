#### **Почему при переключении процесса «чужие записи» в TLB становятся опасными или бессмысленными? ⭐⭐**

Так как каждый процесс имеет собственное изолированное адресное пространство, один и тот же виртуальный адрес (например, 0x1000) в процессе А и процессе Б указывает на разные физические фреймы. Если записи процесса А останутся в TLB после переключения на процесс Б, процессор может по ошибке использовать старую трансляцию и прочитать/записать данные в память процесса А, что является критическим нарушением безопасности и целостности.

---

#### **Какие издержки у полной очистки TLB при каждом контекстном переключении и как это отражается на производительности? ⭐⭐**

Полная очистка (**TLB flush**) заставляет новый процесс начинать работу с абсолютно «пустым» кэшем трансляции. Первые несколько тысяч инструкций вызовут каскад промахов (TLB misses), заставляя систему тратить время на медленные обходы таблиц страниц в ОЗУ. Это создает значительные накладные расходы при каждом переключении контекста, снижая общую отзывчивость системы.

---

#### **Объясните идею ASID/PCID: как маркировка записей позволяет не очищать TLB полностью. ⭐⭐⭐**

Идея заключается в добавлении к каждой записи в TLB небольшого тега — **идентификатора адресного пространства** (ASID или PCID). Теперь TLB может одновременно хранить трансляции для нескольких разных процессов: при поиске адреса MMU сравнивает не только виртуальный номер страницы, но и текущий идентификатор процесса в специальном регистре. Это избавляет от необходимости сбрасывать TLB при каждом переключении задач, позволяя процессам «возвращаться» к уже прогретому кэшу трансляций.

---

#### **Почему многоуровневые TLB (L1/L2) похожи на многоуровневые кеши, но решают отдельную задачу? ⭐⭐**

Они похожи по иерархическому принципу: **L1 TLB** очень маленький и работает со скоростью процессора (1 такт), а **L2 TLB** больше и медленнее (несколько тактов). Однако их задача — не хранение данных (как у L1/L2 Cache), а ускорение **трансляции адресов**. Это отдельный этап конвейера, который должен быть завершен еще до того, как начнется поиск данных в обычных кешах.

---

#### **Какие сценарии приводят к тому, что даже большой L2 TLB не спасает производительность? ⭐⭐⭐**

Производительность падает, если программа работает с данными, разбросанными по огромному количеству страниц (например, обход гигантского графа или разреженной матрицы), так что общее число используемых страниц во много раз превышает емкость даже L2 TLB. Также L2 TLB не спасает, если в системе происходит слишком много переключений между сотнями процессов, которые быстро вытесняют записи друг друга.

---

#### **Почему изменения в PTE «не отражаются мгновенно», и что именно продолжает использовать процессор? ⭐⭐**

Изменения в записи таблицы страниц (**PTE**) происходят в оперативной памяти, но процессор для скорости работы продолжает использовать **старую копию этой записи**, уже загруженную в TLB. До тех пор, пока запись в TLB не будет явно признана недействительной (инвалидирована), MMU будет выдавать старый физический адрес и старые права доступа, игнорируя обновления в ОЗУ.

---

#### **Объясните, почему TLB хранит не только физический адрес, но и права доступа, и как это влияет на безопасность при смене разрешений. ⭐⭐⭐**

TLB кэширует полную запись PTE, включая биты **R/W** (запись), **NX** (выполнение) и **U/S** (пользователь/ядро). Это критично для безопасности: если ОС решила отобрать право на запись в страницу и обновила PTE, но не очистила TLB, процесс сможет продолжать запись в эту страницу, так как аппаратная проверка внутри MMU будет опираться на устаревшие «разрешающие» биты из кэша.

---

#### **Почему правило «делать PTE менее разрешающим без flush нельзя» критично? Приведите логическое обоснование. ⭐⭐⭐**

Логика проста: если вы расширяете права (делаете «более разрешающим»), старая запись в TLB просто вызовет ложный сбой, который ОС легко исправит, обновив кэш. Но если вы сужаете права (например, делаете страницу _Read-Only_), старая запись в TLB будет позволять **нелегальные операции**, о которых ядро даже не узнает, так как процессор не обратится к основной таблице страниц и не увидит изменения. Это создает критическую дыру в защите памяти.

---

#### **Зачем нужен барьер памяти (mfence) между записью нового PTE и инвалидацией TLB? ⭐⭐⭐**

Современные процессоры выполняют инструкции вне очереди (**out-of-order execution**). Без барьера памяти команда инвалидации TLB (`invlpg`) может выполниться физически раньше, чем запись обновленного PTE дойдет до оперативной памяти. В таком случае TLB может мгновенно перезагрузить старое значение из памяти, и инвалидация окажется бесполезной.

---

#### **В чём отличие точечной инвалидации (invlpg) от глобальной (mov cr3, cr3) и как выбрать правильную стратегию? ⭐⭐**

- **invlpg:** удаляет из TLB только одну конкретную запись для указанного адреса. Это быстро и сохраняет кэш для остальных адресов.
- **Глобальная очистка (смена CR3):** полностью сбрасывает весь TLB (если не используются PCID). Стратегия: если изменилась одна страница — выгоднее `invlpg`; если меняется вся карта памяти (смена процесса или масштабная перестройка) — проще выполнить глобальный сброс.

---

#### **Почему в многоядерной системе инвалидация TLB должна выполняться на всех ядрах, где могла кэшироваться страница? ⭐⭐⭐**

В многоядерных системах у каждого ядра свой **локальный TLB**. Если ядро 0 изменило PTE для общей страницы памяти, ядра 1 и 2 об этом не узнают и будут продолжать использовать свои устаревшие копии трансляции из своих кэшей. Это приведет к нарушению когерентности памяти и непредсказуемым сбоям программ, работающих на разных ядрах.

---

#### **Объясните механизм TLB shootdown: зачем нужны IPI и подтверждения от ядер. ⭐⭐⭐**

**TLB shootdown** — это процесс синхронизации кэшей трансляции. Когда одно ядро меняет PTE, оно рассылает остальным ядрам **межпроцессорное прерывание (IPI)**. Получив сигнал, каждое ядро приостанавливает работу, сбрасывает нужную запись в своем TLB и отправляет подтверждение. Только после получения подтверждений от всех ядер инициатор может быть уверен, что старых копий PTE в системе больше нет.

---

#### **Почему нельзя переиспользовать физический фрейм до завершения shootdown, даже если «мы уже обновили PTE»? ⭐⭐⭐**

Пока **shootdown** не завершен, некоторые ядра могут продолжать считать, что по данному виртуальному адресу находится старый физический фрейм, и выполнять в него запись. Если ОС отдаст этот фрейм новому процессу до очистки всех TLB, старый процесс может случайно записать свои данные поверх данных нового процесса, вызвав порчу памяти или нарушение безопасности.

---

#### **Почему обработчик page fault сам подвержен риску page fault, и какие ресурсы ему критически нужны (стек, таблицы, буферы)? ⭐⭐⭐**

Обработчик является обычным кодом ядра, который должен находиться в виртуальной памяти, чтобы процессор мог его исполнить. Если код обработчика, его стек или системные таблицы, к которым он обращается, сами окажутся выгружены на диск, возникнет «рекурсивная» ошибка — **page fault внутри page fault**. Это требует наличия в памяти критического набора ресурсов: кода обработчика, стека ядра, корня таблиц страниц и буферов для работы с диском.

---

#### **Что такое double fault (в смысле «исключение во время обработки исключения») и почему он часто заканчивается фатально для системы? ⭐⭐⭐**

**Double fault** возникает, когда процессор обнаруживает второе исключение в процессе попытки вызвать обработчик первого. Поскольку система не смогла даже начать обработку первичной ошибки (например, из-за отсутствия стека или самого кода обработчика), она переходит в режим критического сбоя. Обычно это заканчивается немедленной перезагрузкой или «синим экраном», так как дальнейшее выполнение инструкций становится невозможным.

---

#### **Какие практики ОС используются, чтобы избежать page fault внутри обработчика (pinning/wired pages, prefaulting, отдельные пулы)? ⭐⭐⭐**

1. **Pinning / Wired pages:** закрепление критически важных страниц ядра в физической ОЗУ с запретом их вытеснения на диск.
2. **Prefaulting:** принудительное обращение к нужным страницам заранее, чтобы убедиться, что они загружены в память и биты присутствия установлены в 1.
3. **Отдельные пулы фреймов:** ядро держит резервный запас физической памяти специально для своих нужд, который никогда не отдается под пользовательские данные.

---

#### **Почему разделение пулов страниц ядра и пользователя снижает вероятность катастрофических сбоев при вытеснении? ⭐⭐⭐**

Это гарантирует, что даже при экстремальной нехватке памяти и интенсивном вытеснении (**thrashing**) пользовательские приложения не смогут «вытеснить» из ОЗУ жизненно важные части самой операционной системы. Разделение ресурсов создает барьер, благодаря которому ядро всегда имеет доступ к своему коду и таблицам, сохраняя способность управлять системой и корректно обрабатывать ошибки страниц.